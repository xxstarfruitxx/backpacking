{
    "name_en": "English (US)",
    "name_local": "English (US)",
    "keys":
    {
        "Quick Tools": "",
        "Error": "",
        "Generate": "",
        "Utilities": "",
        "User": "",
        "Server": "",
        "Clear Batch": "",
        "Negative Prompt": "",
        "Clear Images": "",
        "Like the input prompt text, but describe what NOT to generate.\nTell the AI things you don't want to see.": "",
        "Examples": "",
        "ugly, bad, gross": "",
        "lowres, low quality": "",
        "ReVision Strength": "",
        "How strong to apply ReVision image inputs.\nSet to 0 to disable ReVision processing.": "",
        "ReVision Zero Prompt": "",
        "Zeroes the prompt and negative prompt for ReVision inputs.\nApplies only to the base, the refiner will still get prompts.\nIf you want zeros on both, just delete your prompt text.\nIf not checked, empty prompts will be zeroed regardless.": "",
        "Use IP-Adapter": "",
        "Use IP-Adapter for ReVision input handling.": "",
        "IP-Adapter Weight": "",
        "Weight to use with IP-Adapter (if enabled).": "",
        "How many images to generate at once.": "",
        "Seed": "",
        "Image seed.\n-1 = random.\nDifferent seeds produce different results for the same prompt.": "",
        "...": "",
        "Steps": "",
        "How many times to run the model.\nMore steps = better quality, but more time.\n20 is a good baseline for speed, 40 is good for maximizing quality.\nYou can go much higher, but it quickly becomes pointless above 70 or so.": "",
        "CFG Scale": "",
        "How strongly to scale prompt input.\nHigher CFG scales tend to produce more contrast, and lower CFG scales produce less contrast.\nToo-high values can cause corrupted/burnt images, too-low can cause nonsensical images.\n7 is a good baseline. Normal usages vary between 5 and 9.": "",
        "Variation Seed": "",
        "Image-variation seed.\nCombined partially with the original seed to create a similar-but-different image for the same seed.\n-1 = random.": "",
        "Variation Seed Strength": "",
        "How strongly to apply the variation seed.\n0 = don't use, 1 = replace the base seed entirely. 0.5 is a good value.": "",
        "Aspect Ratio": "",
        "Image aspect ratio. Some models can stretch better than others.": "",
        "Width": "",
        "Image width, in pixels.\nSDv1 uses 512, SDv2 uses 768, SDXL prefers 1024.\nSome models allow variation within a range (eg 512 to 768) but almost always want a multiple of 64.": "",
        "Height": "",
        "Image height, in pixels.\nSDv1 uses 512, SDv2 uses 768, SDXL prefers 1024.\nSome models allow variation within a range (eg 512 to 768) but almost always want a multiple of 64.": "",
        "Init Image": "",
        "Init-image, to edit an image using diffusion.\nThis process is sometimes called 'img2img' or 'Image To Image'.": "",
        "Init Image Creativity": "",
        "Higher values make the generation more creative, lower values follow the init image closer.\nSometimes referred to as 'Denoising Strength' for 'img2img'.": "",
        "Init Image Reset To Norm": "",
        "Merges the init image towards the latent norm.\nThis essentially lets you boost 'init image creativity' past 1.0.\nSet to 0 to disable.": "",
        "Mask Image": "",
        "Mask-image, white pixels are changed, black pixels are not changed, gray pixels are half-changed.": "",
        "Unsampler Prompt": "",
        "If enabled, feeds this prompt to an unsampler before resampling with your main prompt.\nThis is powerful for controlled image editing.": "",
        "Refiner Model": "",
        "The model to use for refinement. This should be a model that's good at small-details, and use a structural model as your base model.\n'Use Base' will use your base model rather than switching.\nSDXL 1.0 released with an official refiner model.": "",
        "Refiner VAE": "",
        "Optional VAE replacement for the refiner stage.": "",
        "Refiner Control Percentage": "",
        "Higher values give the refiner more control, lower values give the base more control.\nThis is similar to 'Init Image Creativity', but for the refiner. This controls how many steps the refiner takes.": "",
        "Refiner Method": "",
        "How to apply the refiner. Different methods create different results.\n'PostApply' runs the base in full, then runs the refiner with an Init Image.\n'StepSwap' swaps the model after x steps during generation.\n'StepSwapNoisy' is StepSwap but with first-stage noise only.": "",
        "Refiner Upscale": "",
        "Optional upscale of the image between the base and refiner stage.\nSometimes referred to as 'high-res fix'.\nSetting to '1' disables the upscale.": "",
        "Refiner Save Before Refine": "",
        "If enabled, saves a copy of the image before the refiner stage.": "",
        "Refiner Upscale Method": "",
        "How to upscale the image, if upscaling is used.": "",
        "Refiner HyperTile": "",
        "The size of hypertiles to use for the refining stage.\nHyperTile is a technique to speed up sampling of large images by tiling the image and batching the tiles.\nThis is useful when using SDv1 models as the refiner. SDXL-Base models do not benefit as much.": "",
        "ControlNet Image Input": "",
        "The image to use as the input to ControlNet guidance.\nThis image will be preprocessed by the chosen preprocessor.\nIf ControlNet is enabled, but this input is not, Init Image will be used instead.": "",
        "ControlNet Preprocessor": "",
        "The preprocessor to use on the ControlNet input image.\nIf toggled off, will be automatically selected.\nUse 'None' to disable preprocessing.": "",
        "ControlNet Model": "",
        "The ControlNet model to use.": "",
        "ControlNet Strength": "",
        "Higher values make the ControlNet apply more strongly. Weaker values let the prompt overrule the ControlNet.": "",
        "ControlNet Start": "",
        "When to start applying controlnet, as a fraction of steps.\nFor example, 0.5 starts applying halfway through. Must be less than End.\nExcluding early steps reduces the controlnet's impact on overall image structure.": "",
        "ControlNet End": "",
        "When to stop applying controlnet, as a fraction of steps.\nFor example, 0.5 stops applying halfway through. Must be greater than Start.\nExcluding later steps reduces the controlnet's impact on finer details.": "",
        "Video Model": "",
        "The model to use for video generation.\nThis should be an SVD (Stable Video Diffusion) model.\nNote that SVD favors a low CFG (~2.5).": "",
        "How many frames to generate within the video.": "",
        "Video FPS": "",
        "The FPS (frames per second) to use for video generation.\nThis configures the target FPS the video will try to generate for.": "",
        "Video Steps": "",
        "How many steps to use for the video model.\nHigher step counts yield better quality, but much longer generation time.\n20 is a good baseline.": "",
        "Video CFG": "",
        "The CFG Scale to use for video generation.\nVideos start with this CFG on the first frame, and then reduce to MinCFG (normally 1) by the end frame.\nSVD-XT 0.9 normally uses 25 frames, and SVD (non-XT) 0.9 uses 14 frames.": "",
        "Video Min CFG": "",
        "The minimum CFG to use for video generation.\nVideos start with max CFG on first frame, and then reduce to this CFG. Set to -1 to disable.": "",
        "Which trained 'motion bucket' to use for the video model.\nHigher values induce more motion. Most values should stay in the 100-200 range.\n127 is a good baseline.": "",
        "Video Preview Type": "",
        "How to display previews for generating videos.\n'Animate' shows a low-res animated video preview.\n'iterate' shows one frame at a time while it goes.\n'one' displays just the first frame.\n'none' disables previews.": "",
        "Video Augmentation Level": "",
        "How much noise to add to the init image.\nHigher values yield more motion.": "",
        "Video Format": "",
        "What format to save videos in.": "",
        "[AutoWebUI] Sampler": "",
        "Sampler type (for AutoWebUI)": "",
        "[ComfyUI] Custom Workflow": "",
        "What custom workflow to use in ComfyUI (built in the Comfy Workflow Editor tab)": "",
        "Enable AITemplate": "",
        "If checked, enables AITemplate for ComfyUI generations (UNet only). Only compatible with some GPUs.": "",
        "Sampler": "",
        "Sampler type (for ComfyUI)": "",
        "Scheduler": "",
        "Scheduler type (for ComfyUI)": "",
        "[SAPI] Engine": "",
        "Engine for StabilityAPI to use.": "",
        "[SAPI] Sampler": "",
        "Sampler for StabilityAPI to use.": "",
        "Batch Size": "",
        "Batch size - generates more images at once on a single GPU.\nThis increases VRAM usage.\nMay in some cases increase overall speed by a small amount (runs slower to get the images, but slightly faster per-image).": "",
        "Alt Resolution Height Multiplier": "",
        "When enabled, the normal width parameter is used, and this value is multiplied by the width to derive the image height.": "",
        "Do Not Save": "",
        "If checked, tells the server to not save this image.\nUseful for quick test generations, or 'generate forever' usage.": "",
        "No Previews": "",
        "If checked, tells the server that previews are not desired.\nMay make generations slightly faster in some cases.": "",
        "[Internal] Backend Type": "",
        "Which StableSwarmUI backend type should be used for this request.": "",
        "Manually force a specific exact backend (by ID #) to be used for this generation.": "",
        "Wildcard Seed": "",
        "Wildcard selection seed.\nIf enabled, this seed will be used for selecting entries from wildcards.\nIf disabled, the image seed will be used.\n-1 = random.": "",
        "No Seed Increment": "",
        "If checked, the seed will not be incremented when Images is above 1.\nUseful for example to test different wildcards for the same seed rapidly.": "",
        "Personal Note": "",
        "Optional field to type in any personal text note you want.\nThis will be stored in the image metadata.": "",
        "Global Region Factor": "",
        "When using regionalized prompts, this factor controls how strongly the global prompt overrides the regional prompts.\n0 means ignore global prompt, 1 means ignore regional, 0.5 means half-n-half.": "",
        "Regional Object Inpainting Model": "",
        "When using regionalized prompts with distinct 'object' values, this overrides the model used to inpaint those objects.": "",
        "ReVision Model": "",
        "The CLIP Vision model to use for ReVision inputs.\nThis will also override IPAdapter (if IPAdapter-G is in use).": "",
        "VAE": "",
        "The VAE (Variational Auto-Encoder) controls the translation between images and latent space.\nIf your images look faded out, or glitched, you may have the wrong VAE.\nAll models have a VAE baked in by default, this option lets you swap to a different one if you want to.": "",
        "What layer of CLIP to stop at, from the end.\nAlso known as 'CLIP Skip'. Default CLIP Skip is -1, some models prefer -2.\nSDv2, SDXL, and beyond do not need this set ever.": "",
        "Remove Background": "",
        "If enabled, removes the background from the generated image.\nThis uses RemBG.": "",
        "Rho value for the sampler.\nOnly applies to Karras/Exponential schedulers.": "",
        "Maximum sigma value for the sampler.\nOnly applies to Karras/Exponential schedulers.": "",
        "Minimum sigma value for the sampler.\nOnly applies to Karras/Exponential schedulers.": "",
        "Seamless Tileable": "",
        "Makes the generated image seamlessly tileable (like a 3D texture would be).": "",
        "Self-Attention Guidance Scale": "",
        "Scale for Self-Attention Guidance.\n''Self-Attention Guidance (SAG) uses the intermediate self-attention maps of diffusion models to enhance their stability and efficacy.\nSpecifically, SAG adversarially blurs only the regions that diffusion models attend to at each iteration and guides them accordingly.''\nDefaults to 0.5.": "",
        "Self-Attention Guidance Sigma Blur": "",
        "Blur-sigma for Self-Attention Guidance.\nDefaults to 2.0.": "",
        "If enabled, decodes images through the VAE using tiles of this size.\nVAE Tiling reduces VRAM consumption, but takes longer and may impact quality.": "",
        "[Dynamic Thresholding]\nMimic Scale value (target for the CFG Scale recentering).": "",
        "[Dynamic Thresholding]\nthresholding percentile. '1' disables, '0.95' is decent value for enabled.": "",
        "[DT] CFG Scale Mode": "",
        "[Dynamic Thresholding]\nMode for the CFG Scale scheduler.": "",
        "[Dynamic Thresholding]\nCFG Scale minimum value (for non-constant CFG mode).": "",
        "[DT] Mimic Scale Mode": "",
        "[Dynamic Thresholding]\nMode for the Mimic Scale scheduler.": "",
        "[Dynamic Thresholding]\nMimic Scale minimum value (for non-constant mimic mode).": "",
        "[Dynamic Thresholding]\nIf either scale scheduler is 'Power', this is the power factor.\nIf using 'repeating', this is the number of repeats per image. Otherwise, it does nothing.": "",
        "[DT] Separate Feature Channels": "",
        "[Dynamic Thresholding]\nWhether to separate the feature channels.\nNormally leave this on. I think it should be off for RCFG?": "",
        "[DT] Scaling Startpoint": "",
        "[Dynamic Thresholding]\nWhether to scale relative to the mean value or to zero.\nUse 'MEAN' normally. If you want RCFG logic, use 'ZERO'.": "",
        "[DT] Variability Measure": "",
        "[Dynamic Thresholding]\nWhether to use standard deviation ('STD') or thresholded absolute values ('AD').\nNormally use 'AD'. Use 'STD' if wanting RCFG logic.": "",
        "[Dynamic Thresholding]\n'phi' interpolation factor.\nInterpolates between original value and DT value, such that 0.0 = use original, and 1.0 = use DT.\n(This exists because RCFG is bad and so half-removing it un-breaks it - better to just not do RCFG).": "",
        "[FreeU] Apply To": "",
        "Which models to apply FreeU to, as base, refiner, or both. Irrelevant when not using refiner.": "",
        "Block1 multiplier value for FreeU.\nPaper recommends 1.1.": "",
        "Block2 multiplier value for FreeU.\nPaper recommends 1.2.": "",
        "Skip1 multiplier value for FreeU.\nPaper recommends 0.9.": "",
        "Skip2 multiplier value for FreeU.\nPaper recommends 0.2.": "",
        "Automatic Scorer": "",
        "Scoring engine(s) to use when scoring this image. Multiple scorers can be used and will be averaged together. Scores are saved in image metadata.": "",
        "Only keep images with a generated score above this minimum.": "",
        "Only keep the best *this many* images in a batch based on scoring.\n(For example, if batch size = 8, and this value = 2, then 8 images will generate and will be scored, and the 2 best will be kept and the other 6 discarded.)": "",
        "Prompt": "",
        "The input prompt text that describes the image you want to generate.\nTell the AI what you want to see.": "",
        "a photo of a cat": "",
        "a cartoonish drawing of an astronaut": "",
        "Prompt Images": "",
        "Images to include with the prompt, for eg ReVision or UnCLIP.\nIf this parameter is visible, you've done something wrong - this parameter is tracked internally.": "",
        "[Grid Gen] Presets": "",
        "Apply parameter presets to the image. Can use a comma-separated list to apply multiple per-cell, eg 'a, b || a, c || b, c'": "",
        "[Grid Gen] Prompt Replace": "",
        "Replace text in the prompt (or negative prompt) with some other text.": "",
        "ControlNet Preview Only": "",
        "(For API usage) If enabled, requests preview output from ControlNet and no image generation at all.": "",
        "Debug Regional Prompting": "",
        "If checked, outputs masks from regional prompting for debug reasons.": "",
        "Percentage of steps to cut off before the image is done generation.": "",
        "Model": "",
        "What main checkpoint model should be used.": "",
        "LoRA Weights": "",
        "Weight values for the LoRA model list.": "",
        "LoRAs": "",
        "LoRAs (Low-Rank-Adaptation Models) are a way to customize the content of a model without totally replacing it.\nYou can enable one or several LoRAs over top of one model.": "",
        "[ComfyUI] Workflow": "",
        "What hand-written specialty workflow to use in ComfyUI (files in 'Workflows' folder within the ComfyUI extension)": "",
        "If enabled, automatically clears out the image batch when generating a new one. If disabled, leaves history to grow until you refresh the page.": "",
        "If enabled, automatically swaps to previews of new images the moment they're available, before the image itself is done generating.": "",
        "Auto Swap To Previews": "",
        "Tell the AI what you want to see, then press Enter to submit.\nConsider 'a photo of a cat', or 'cartoonish drawing of an astronaut'": "",
        "Start generating images\nRight-click for advanced options.": "",
        "Generate Forever": "",
        "Generate Previews": "",
        "Interrupt Current Session": "",
        "Interrupt All Sessions": "",
        "Show Prompt Tokenization": "",
        "Reset Page Layout": "",
        "Reload Parameter Values": "",
        "Reset Params to Default": "",
        "Open Empty Image Editor": "",
        "Reshow Welcome Message": "",
        "Missing Preview Preset": "",
        "Start Anyway": "",
        "Cancel": "",
        "Current presets": "",
        "Current LoRAs: ": "",
        "Image History": "",
        "Presets": "",
        "Models": "",
        "VAEs": "",
        "Embeddings": "",
        "ControlNets": "",
        "Wildcards": "",
        "Tools": "",
        "[Close]\n                    ": "",
        "Export Presets": "",
        "StableSwarmUI JSON": "",
        "Prompt only CSV": "",
        "Download": "",
        "Close": "",
        "Import Presets": "",
        "Upload Preset File (JSON or CSV)": "",
        "Overwrite existing presets": "",
        "Import": "",
        "Create New Preset: ": "",
        "Preset description text...": "",
        "Enable/disable image": "",
        "Parameters in the Preset": "",
        "Select the checkbox next to inputs you want included in the preset.By default this will override values entirely. For text inputs, use {value} to include the current UI value instead of overriding.": "",
        "Display Advanced Options?": "",
        "Display Normally-Hidden Options?": "",
        "Save": "",
        "Edit Preset": "",
        "Delete Preset": "",
        "Input Folder": "",
        "Output Folder": "",
        "Use As Init": "",
        "Use As ControlNet Input": "",
        "Use As ReVision": "",
        "Grid Generator: Load Modal": "",
        "Output Folder Name": "",
        "Overwrite Existing Files": "",
        "Fast Skip": "",
        "Generate Page": "",
        "Publish Generation Metadata": "",
        "Dry Run": "",
        "Allow Reordering": "",
        "Continue On Error": "",
        "Edit Model Metadata: ": "",
        "Model Author": "",
        "Model Type": "",
        "Resolution, eg 1024x1024": "",
        "Model License": "",
        "Model Date": "",
        "Model Merged From": "",
        "Model Tags": "",
        "Model Usage Hint": "",
        "Model Trigger Phrase": "",
        "Model description text...": "",
        "Edit Wildcard: ": "",
        "Wildcards are lists of random prompt segments. One entry per line. Prompt sub-syntax is allowed (eg you can link another wildcard, or use <random:...>, or <preset:...> or anything else you want).": "",
        "Prefix a line with # to make it a comment (ie won't be counted as an option).": "",
        "Test Wildcard: ": "",
        "Test Again": "",
        "Info": "",
        "CLIP Tokenization": "",
        "Pickle To Safetensors": "",
        "LoRA Extractor": "",
        "\n                            The \"Utilities\" tab provides a variety of general utilities and tools in the subtabs above, and the quicktools below.\n                        ": "",
        "Metadata Reset": "",
        "Reset All Metadata": "",
        "CLIP Tokenizer": "",
        "\n                            This is a tool to analyze CLIP tokenization for Stable Diffusion models.\n                            All current Stable Diffusion models use the same CLIP token set, so this applies to them all.\n                            Simply type some text in the box below to see how it gets tokenized.\n                            It will show each text-piece, and its numerical ID.\n                        ": "",
        "\n                            This is a tool to quickle convert legacy Pickle (.pt, .ckpt, .bin) files to modern Safetensors files.\n                            WARNING: Pickle files may contain malicious code. Do not use this tool or otherwise load pickle files unless you trust their source.\n                            The pickle files will be moved to a \"backups\" folder, and the safetensors files left in place where the pickles originally were.\n                            You may delete the backups folder after confirming the new safetensors files work as intended.\n                            Be aware that this may temporarily use a large amount of filespace.\n                            This may take some time to process.\n                            (You can continue using the UI as normal while this runs)\n                        ": "",
        "Convert to FP16?": "",
        "\n                            TypeCountConvert\n                        ": "",
        "Convert Models": "",
        "Convert LoRAs": "",
        "Convert VAEs": "",
        "Convert Embeddings": "",
        "Convert ControlNets": "",
        "\n                            This is a tool to extract a LoRA from the difference between two models.\n                            \"Base\" should be whatever the common base model is, eg SDXL-1.0-Base.\n                            \"Other\" should be the unique model with information to extract into the LoRA.\n                            The closer Base is to Other, the less complex the LoRA's data will be, and the more likely it will work well with other models that were built off the same base.\n                            \n                            Note that LoRA Extraction is an imperfect partial process, and will lose some of the data that makes the model unique.\n                            Rank is a number indicating how much detail to try to save. Higher numbers result in bigger files, and only slightly more accurate matching. Small values (eg 16) are usually sufficient.\n                        ": "",
        "Extract LoRA": "",
        "User Info": "",
        "User Settings": "",
        "Parameter Configuration": "",
        "\n                (TODO: info n stuff here)\n                \n            ": "",
        "SaveFiles": "",
        "Whether your files save to server data drive or not.": "",
        "StarNoFolders": "",
        "If true, folders will be discard from starred image paths.": "",
        "Theme": "",
        "What theme to use. Default is 'dark_dreams'.": "",
        "ResetBatchSizeToOne": "",
        "If enabled, batch size will be reset to 1 when parameters are loaded.\nThis can prevent accidents that might thrash your GPU or cause compatibility issues, especially for example when importing a comfy workflow.\nYou can still set the batch size at will in the GUI.": "",
        "HintFormat": "",
        "The format for parameter hints to display as.\nDefault is 'BUTTON'.": "",
        "When generating live previews, this is how many simultaneous generation requests can be waiting at one time.": "",
        "Set to a number above 1 to allow generations of multiple images to automatically generate square mini-grids when they're done.": "",
        "How many images the history view should stop trying to load after.": "",
        "ImageHistoryUsePreviews": "",
        "If true, the Image History view will cache small preview thumbnails of images.\nThis should make things run faster. You can turn it off if you don't want that.": "",
        "Language": "",
        "What language to display the UI in.\nDefault is 'en' (English).": "",
        "Format": "",
        "Builder for output file paths. Can use auto-filling placeholders like '[model]' for the model name, '[prompt]' for a snippet of prompt text, etc.\nFull details in the docs: https://github.com/Stability-AI/StableSwarmUI/blob/master/docs/User%20Settings.md#path-format": "",
        "How long any one part can be.\nDefault is 40 characters.": "",
        "ImageFormat": "",
        "What format to save images in.\nDefault is '.jpg' (at 100% quality).": "",
        "SaveMetadata": "",
        "Whether to store metadata into saved images.\nDefaults enabled.": "",
        "If set to non-0, adds DPI metadata to saved images.\n'72' is a good value for compatibility with some external software.": "",
        "SaveTextFileMetadata": "",
        "If set to true, a '.txt' file will be saved alongside images with the image metadata easily viewable.\nThis can work even if saving in the image is disabled. Defaults disabled.": "",
        "DefaultSDXLVAE": "",
        "What VAE to use with SDXL models by default. Use 'None' to use the one in the model.": "",
        "DefaultSDv1VAE": "",
        "What VAE to use with SDv1 models by default. Use 'None' to use the one in the model.": "",
        "Notice: this is raw internal configuration of parameters. Don't mess with this unless you know what you're doing.": "",
        "Server Info": "",
        "Backends": "",
        "Server Configuration": "",
        "Logs": "",
        "Local Network": "",
        "Resource Usage": "",
        "Shutdown": "",
        "Free Memory": "",
        "Show Advanced Backend Types": "",
        "Restart All Backends": "",
        "StartScript": "",
        "ExtraArgs": "",
        "DisableInternalArgs": "",
        "AutoUpdate": "",
        "EnablePreviews": "",
        "Address": "",
        "AllowIdle": "",
        "IsInstalled": "",
        "If this is set to 'true', hides the installer page. If 'false', the installer page will be shown.": "",
        "Ratelimit, in milliseconds, between Nvidia GPU status queries. Default is 1000 ms (1 second).": "",
        "LaunchMode": "",
        "How to launch the UI. If 'none', just quietly launch.\nIf 'web', launch your web-browser to the page.\nIf 'webinstall', launch web-browser to the install page.\nIf 'electron', launch the UI in an electron window (NOT YET IMPLEMENTED).": "",
        "LogLevel": "",
        "The minimum tier of logs that should be visible in the console.\nDefault is 'info'.": "",
        "ModelRoot": "",
        "Root path for model files. Use a full-formed path (starting with '/' or a Windows drive like 'C:') to use an absolute path.\nDefaults to 'Models'.": "",
        "SDModelFolder": "",
        "The model folder to use within 'ModelRoot'.\nDefaults to 'Stable-Diffusion'.\nAbsolute paths work too.": "",
        "SDLoraFolder": "",
        "The LoRA (or related adapter type) model folder to use within 'ModelRoot'.\nDefaults to 'Lora'.\nAbsolute paths work too.": "",
        "SDVAEFolder": "",
        "The VAE (autoencoder) model folder to use within 'ModelRoot'.\nDefaults to 'VAE'.\nAbsolute paths work too.": "",
        "SDEmbeddingFolder": "",
        "The Embedding (eg textual inversion) model folder to use within 'ModelRoot'.\nDefaults to 'Embeddings'.\nAbsolute paths work too.": "",
        "SDControlNetsFolder": "",
        "The ControlNets model folder to use within 'ModelRoot'.\nDefaults to 'controlnet'.\nAbsolute paths work too.": "",
        "SDClipVisionFolder": "",
        "The CLIP Vision model folder to use within 'ModelRoot'.\nDefaults to 'clip_vision'.\nAbsolute paths work too.": "",
        "DataPath": "",
        "Root path for data (user configs, etc).\nDefaults to 'Data'": "",
        "OutputPath": "",
        "Root path for output files (images, etc).\nDefaults to 'Output'": "",
        "WildcardsFolder": "",
        "The folder for wildcard (.txt) files, under Data.\nDefaults to 'Wildcards'": "",
        "AppendUserNameToOutputPath": "",
        "When true, output paths always have the username as a folder.\nWhen false, this will be skipped.\nKeep this on in multi-user environments.": "",
        "Host": "",
        "What web host address to use. `localhost` means your PC only.\nLinux users may use `0.0.0.0` to mean accessible to anyone that can connect to your PC (ie LAN users, or the public if your firewall is open).\nWindows users may use `*` for that, though it may require additional Windows firewall configuration.\nAdvanced server users may wish to manually specify a host bind address here.": "",
        "What web port to use. Default is '7801'.": "",
        "PortCanChange": "",
        "If true, if the port is already in use, the server will try to find another port to use instead.\nIf false, the server will fail to start if the port is already in use.": "",
        "Backends are automatically assigned unique ports. This value selects which port number to start the assignment from.\nDefault is '7820'.": "",
        "How many directories deep a user's custom OutPath can be.\nDefault is 5.": "",
        "AllowedSettings": "",
        "Which user-settings the user is allowed to modify.\nDefault is all of them.": "",
        "Admin": "",
        "If true, the user is treated as a full admin.\nThis includes the ability to modify these settings.": "",
        "CanChangeModels": "",
        "If true, user may load models.\nIf false, they may only use already-loaded models.": "",
        "AllowedModels": "",
        "What models are allowed, as a path regex.\nDirectory-separator is always '/'. Can be '.*' for all, 'MyFolder/.*' for only within that folder, etc.\nDefault is all.": "",
        "PermissionFlags": "",
        "Generic permission flags. '*' means all.\nDefault is all.": "",
        "How many images can try to be generating at the same time on this user.": "",
        "How many times to retry initializing a backend before giving up. Default is 3.": "",
        "Safety check, the maximum duration all requests can be waiting for a backend before the system declares a backend handling failure.": "",
        "The maximum duration an individual request can be waiting on a backend to be available before giving up.\nNot to be confused with 'MaxTimeoutMinutes' which requires backends be unresponsive for that duration, this duration includes requests that are merely waiting because other requests are queued.\nDefaults to 60 * 24 * 7 = 1 week (ultra-long max queue duration).": "",
        "The maximum number of pending requests to continue forcing orderly processing of.\nOver this limit, requests may start going out of order.": "",
        "How many minutes to wait after the last generation before automatically freeing up VRAM (to prevent issues with other programs).\nThis has the downside of a small added bit of time to load back onto VRAM at next usage.\nUse a decimal number to free after seconds.\nDefaults to 10 minutes.": "",
        "How many minutes to wait after the last generation before automatically freeing up system RAM (to prevent issues with other programs).\nThis has the downside of causing models to fully load from data drive at next usage.\nUse a decimal number to free after seconds.\nDefaults to 60 minutes (one hour).": "",
        "OverrideWelcomeMessage": "",
        "Optionally specify a (raw HTML) welcome message here. If specified, will override the automatic welcome messages.": "",
        "QueueStartWebhook": "",
        "Webhook to call (empty JSON POST) when queues are starting up from idle.\nLeave empty to disable any webhook.\nCall must return before the first generation starts.": "",
        "QueueEndWebhook": "",
        "Webhook to call (empty JSON POST) when all queues are done and the server is going idle.\nLeave empty to disable any webhook.\nCall must return before queuing may restart.": "",
        "How long to wait (in seconds) after all queues are done before sending the queue end webhook.\nThis is useful to prevent rapid start+end calls.": "",
        "Failed to get WebSocket address. You may be connecting to the server in an unexpected way. Please use \"http\" or \"https\" URLs.": "",
        "Failed to get session ID after 3 tries. Your account may have been invalidated. Try refreshing the page, or contact the site owner.": "",
        "Failed to send request to server. Did the server crash?": "",
        "The server has updated since you opened the page, please refresh.": ""
    }
}
